{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pvdeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Points\n",
    "\n",
    "We are going to add all of the points in the American West to the scenario and downsample by a factor of 1. This will include only half of the points in the latitude axis and half in the longitude axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_points = pvdeg.GeospatialScenario(name=\"dynamic-selection\")\n",
    "\n",
    "dynamic_points.addLocation(\n",
    "    state=[\"CO\", \"UT\"],  # , 'NM', 'NV', 'ID', 'WY', 'AZ', 'CA', 'OR', 'WA'],\n",
    "    downsample_factor=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview The Scenario's Points\n",
    "\n",
    "Use `plot_cords` to get a quick snapshot of all coordinates included in the scenario's metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_points.plot_coords(\n",
    "    coord_1=[48.574790, -130.253906],  # uncomment to see Larger scale view\n",
    "    coord_2=[25.482951, -68.027344],\n",
    "    size=0.005,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_points.meta_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downselecting\n",
    "\n",
    "Using weighted random choices based on elevation we will create a sparse grid from the full metadata for fast calculations. This requires sklearn to be installed but this is not in the `pvdeg` dependency list to you will have to install it seperately.\n",
    "\n",
    "### Normalization\n",
    "\n",
    "At each metadata point in our dataset we will calculate a weight based on its changes in elevation compared to its neighbors. The higher the weight, the greater the change in elevation from a point's immediate neighbors. The downselection methods and functions use these weights to randomly select a subset of the datapoints, prefferentially selecting those with higher weights. \n",
    "\n",
    "We have some control over which points get selected because all points' weights must be normalized (mapped from 0 to 1) before downselecting. We can apply a function such as $e^x$ or $\\log x$ to the weights during normalization. This could help change the distribution of weights that are chosen. This could remove points from the mountains and add them to areas with fewer changes in elevation, or vice versa.\n",
    "\n",
    "*Note: `pvdeg`'s downselection functions use `numpy.random`, the random seed is not fixed so the result will change between runs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Providing a KdTree\n",
    "\n",
    "As shown below the lines to create a kdtree are commented out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# west_tree = pvdeg.geospatial.meta_KDtree(meta_df=dynamic_points.meta_data)\n",
    "\n",
    "dynamic_points.downselect_elevation_stochastic(\n",
    "    # kdtree=west_tree,\n",
    "    downselect_prop=0.1,\n",
    "    normalization=\"linear\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_points.plot_coords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting from Scenario\n",
    "\n",
    "Scenarios provide an easy way to select and downsample geospatial data but we can easily pull out the data to use other `pvdeg` functions on it. In the cell below, we extract the weather data and meta data from the scenario and take only the matching entries from the weather. Then we load the xarray dataset into memory. Previously, it was stored lazily out of memory but we want to do operations on it. (Chunking causes issues when calculating so this eliminates any chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = dynamic_points.weather_data\n",
    "\n",
    "sub_weather = weather.sel(\n",
    "    gid=dynamic_points.meta_data.index\n",
    ")  # downselect weather using restricted metadata set\n",
    "\n",
    "sub_weather = sub_weather.compute()  # load into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geospatial Calculation\n",
    "\n",
    "Run a standoff calculation on the extracted scenario weather data and scenario meta data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geospatial analysis now\n",
    "\n",
    "geo = {\n",
    "    \"func\": pvdeg.standards.standoff,\n",
    "    \"weather_ds\": sub_weather,\n",
    "    \"meta_df\": dynamic_points.meta_data,\n",
    "}\n",
    "\n",
    "analysis_result = pvdeg.geospatial.analysis(**geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing Results\n",
    "\n",
    "Inspecting the xarray dataset below shows us that we have many Not a Number (NaN) entries. These occur because we did not provide weather data at every point in the grid of possile latitude-longitude pairs. Expanding the `x` datavariable shows that there are some valid results but these are uncommon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Sparse Data I\n",
    "\n",
    "If we try to plot existing data with the current plotting methods exposed by `pvdeg` we will encounter issues. This will produce weak plotting results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvdeg.geospatial.plot_USA(analysis_result[\"x\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Sparse Data II\n",
    "\n",
    "Utilize the new `plot_sparse_analysis` function below to interpolate and plot a solid color map. We can use different interpolation schemes but `nearest` and `linear` are best. See [scipy.interpolate.griddata](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.griddata.html) for more information about interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvdeg.geospatial.plot_sparse_analysis(analysis_result, data_var=\"x\", method=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
